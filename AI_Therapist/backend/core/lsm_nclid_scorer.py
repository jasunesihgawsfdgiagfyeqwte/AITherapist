import pandas as pd
import numpy as np
import jieba
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from typing import Dict, List, Any, Optional
import sqlite3
import json
from datetime import datetime, timedelta


# ============ è¯„åˆ†ç³»ç»Ÿæ ¸å¿ƒç±» ============

class LSMNCLIDScorer:
    """LSM & nCLiD è¯„åˆ†å™¨ - é›†æˆåˆ°LangGraphå·¥ä½œæµ"""

    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.stopwords = ['çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å°±', 'ä¸', 'åœ¨', 'å•Š', 'å•¦', 'å˜›', 'å‘¢']

        # ç¼“å­˜ç”¨æˆ·çš„å¯¹è¯å†å²å‘é‡
        self.user_vectors_cache = {}
        self.ai_vectors_cache = {}

    def get_function_words(self, text: str) -> List[str]:
        """æå–åŠŸèƒ½è¯"""
        tokens = jieba.lcut(str(text))
        return [t for t in tokens if t in self.stopwords]

    def calc_lsm_single(self, user_text: str, ai_text: str) -> float:
        """è®¡ç®—å•è½®LSMåˆ†æ•°"""
        words1 = self.get_function_words(user_text)
        words2 = self.get_function_words(ai_text)

        if not words1 and not words2:
            return 0.0

        overlap = len(set(words1) & set(words2))
        denom = len(set(words1) | set(words2)) + 1e-6
        return round(overlap / denom, 4)

    def calc_nclid_single(self, user_text: str, ai_text: str) -> float:
        """è®¡ç®—å•è½®nCLiDåˆ†æ•°"""
        user_vec = self.model.encode([user_text])
        ai_vec = self.model.encode([ai_text])

        # è®¡ç®—ä½™å¼¦è·ç¦» (1 - ä½™å¼¦ç›¸ä¼¼åº¦)
        similarity = cosine_similarity(user_vec, ai_vec)[0][0]
        distance = 1 - similarity
        return round(distance, 4)

    def calc_lsm_cumulative(self, user_texts: List[str], ai_texts: List[str]) -> float:
        """è®¡ç®—ç´¯ç§¯LSMåˆ†æ•°"""
        user_corpus = " ".join(user_texts)
        ai_corpus = " ".join(ai_texts)
        return self.calc_lsm_single(user_corpus, ai_corpus)

    def calc_nclid_weighted(self, user_texts: List[str], ai_texts: List[str]) -> float:
        """è®¡ç®—åŠ æƒnCLiDåˆ†æ•°ï¼ˆé€è½®ç´¯ç§¯ï¼‰"""
        if not user_texts or not ai_texts:
            return 0.0

        min_len = min(len(user_texts), len(ai_texts))
        if min_len == 0:
            return 0.0

        weighted_score = 0.0
        for i in range(min_len):
            current_distance = self.calc_nclid_single(user_texts[i], ai_texts[i])

            if i == 0:
                weighted_score = current_distance
            else:
                # åŠ æƒå¹³å‡ï¼šå½“å‰è·ç¦»ä¸å†å²å¹³å‡çš„å‡å€¼
                weighted_score = (current_distance + weighted_score) / 2

        return round(weighted_score, 4)

    def score_conversation(self, user_id: str, user_input: str, ai_response: str) -> Dict[str, float]:
        """ä¸ºå•æ¬¡å¯¹è¯è¯„åˆ†ï¼Œå¹¶æ›´æ–°ç´¯ç§¯åˆ†æ•°"""

        # è·å–ç”¨æˆ·å†å²å¯¹è¯
        conversation_history = self.get_conversation_history(user_id)

        # è®¡ç®—å•è½®åˆ†æ•°
        single_lsm = self.calc_lsm_single(user_input, ai_response)
        single_nclid = self.calc_nclid_single(user_input, ai_response)

        # æ›´æ–°å†å²è®°å½•
        conversation_history["user_texts"].append(user_input)
        conversation_history["ai_texts"].append(ai_response)

        # è®¡ç®—ç´¯ç§¯åˆ†æ•°
        cumulative_lsm = self.calc_lsm_cumulative(
            conversation_history["user_texts"],
            conversation_history["ai_texts"]
        )
        cumulative_nclid = self.calc_nclid_weighted(
            conversation_history["user_texts"],
            conversation_history["ai_texts"]
        )

        # ä¿å­˜æ›´æ–°çš„å†å²
        self.save_conversation_history(user_id, conversation_history)

        return {
            "single_lsm": single_lsm,
            "single_nclid": single_nclid,
            "cumulative_lsm": cumulative_lsm,
            "cumulative_nclid": cumulative_nclid,
            "total_rounds": len(conversation_history["user_texts"])
        }

    def get_conversation_history(self, user_id: str) -> Dict[str, List[str]]:
        """è·å–ç”¨æˆ·å¯¹è¯å†å²"""
        conn = sqlite3.connect('psychology_ai.db')
        cursor = conn.cursor()

        # è·å–æœ€è¿‘30å¤©çš„å¯¹è¯ï¼ˆé¿å…å†å²è¿‡é•¿å½±å“æ€§èƒ½ï¼‰
        thirty_days_ago = datetime.now() - timedelta(days=30)

        cursor.execute('''
            SELECT user_input, ai_response FROM conversations 
            WHERE user_id = ? AND created_at > ?
            ORDER BY created_at ASC
        ''', (user_id, thirty_days_ago))

        history = cursor.fetchall()
        conn.close()

        return {
            "user_texts": [h[0] for h in history],
            "ai_texts": [h[1] for h in history]
        }

    def save_conversation_history(self, user_id: str, history: Dict[str, List[str]]):
        """ä¿å­˜å¯¹è¯å†å²ï¼ˆè¿™é‡Œå®é™…ä¸Šä¸éœ€è¦é¢å¤–ä¿å­˜ï¼Œå› ä¸ºæ•°æ®åº“å·²æœ‰è®°å½•ï¼‰"""
        # ç”±äºå¯¹è¯å·²ç»ä¿å­˜åœ¨conversationsè¡¨ä¸­ï¼Œè¿™é‡Œåªæ˜¯å ä½ç¬¦
        # å®é™…å®ç°ä¸­å¯ä»¥ç”¨äºç¼“å­˜ä¼˜åŒ–
        pass

    def get_user_score_summary(self, user_id: str, days: int = 7) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·è¯„åˆ†æ‘˜è¦"""
        conn = sqlite3.connect('psychology_ai.db')
        cursor = conn.cursor()

        # è·å–æŒ‡å®šå¤©æ•°å†…çš„è¯„åˆ†æ•°æ®
        since_date = datetime.now() - timedelta(days=days)

        cursor.execute('''
            SELECT lsm_score, nclid_score, understanding_score, created_at
            FROM conversations 
            WHERE user_id = ? AND created_at > ? AND lsm_score IS NOT NULL
            ORDER BY created_at ASC
        ''', (user_id, since_date))

        scores = cursor.fetchall()
        conn.close()

        if not scores:
            return {
                "avg_lsm": 0.0,
                "avg_nclid": 0.0,
                "avg_understanding": 0.0,
                "score_trend": "æ— æ•°æ®",
                "total_conversations": 0
            }

        lsm_scores = [s[0] for s in scores if s[0] is not None]
        nclid_scores = [s[1] for s in scores if s[1] is not None]
        understanding_scores = [s[2] for s in scores if s[2] is not None]

        # è®¡ç®—è¶‹åŠ¿ï¼ˆæœ€è¿‘3æ¬¡vsä¹‹å‰çš„å¹³å‡ï¼‰
        if len(lsm_scores) >= 6:
            recent_avg = np.mean(lsm_scores[-3:])
            previous_avg = np.mean(lsm_scores[:-3])
            trend = "ä¸Šå‡" if recent_avg > previous_avg else "ä¸‹é™"
        else:
            trend = "æ•°æ®ä¸è¶³"

        return {
            "avg_lsm": round(np.mean(lsm_scores), 4) if lsm_scores else 0.0,
            "avg_nclid": round(np.mean(nclid_scores), 4) if nclid_scores else 0.0,
            "avg_understanding": round(np.mean(understanding_scores), 2) if understanding_scores else 0.0,
            "score_trend": trend,
            "total_conversations": len(scores),
            "latest_lsm": lsm_scores[-1] if lsm_scores else 0.0,
            "latest_nclid": nclid_scores[-1] if nclid_scores else 0.0
        }


# ============ é›†æˆåˆ°å·¥ä½œæµçŠ¶æ€ ============

from typing import TypedDict


class ChatStateWithScoring(TypedDict):
    """å¸¦è¯„åˆ†çš„èŠå¤©çŠ¶æ€"""

    # åŸºç¡€å­—æ®µ
    user_id: str
    user_input: str
    character_id: str
    emotion: str
    cbt_stage: int
    ai_response: str
    understanding_score: float

    # è¯„åˆ†å­—æ®µï¼ˆæ–°å¢ï¼‰
    lsm_scores: Dict[str, float]  # LSMè¯„åˆ†
    nclid_scores: Dict[str, float]  # nCLiDè¯„åˆ†
    score_analysis: Dict[str, Any]  # è¯„åˆ†åˆ†æ

    # å…ƒæ•°æ®
    metadata: Optional[Dict[str, Any]]


# ============ é›†æˆåˆ°å·¥ä½œæµèŠ‚ç‚¹ ============

def enhanced_memory_node(state: ChatStateWithScoring) -> ChatStateWithScoring:
    """å¢å¼ºçš„è®°å¿†èŠ‚ç‚¹ - åŒ…å«LSM&nCLiDè¯„åˆ†"""

    # åˆå§‹åŒ–è¯„åˆ†å™¨
    scorer = LSMNCLIDScorer()

    # è®¡ç®—è¯„åˆ†
    scores = scorer.score_conversation(
        user_id=state["user_id"],
        user_input=state["user_input"],
        ai_response=state["ai_response"]
    )

    # æ›´æ–°çŠ¶æ€
    state["lsm_scores"] = {
        "single": scores["single_lsm"],
        "cumulative": scores["cumulative_lsm"]
    }
    state["nclid_scores"] = {
        "single": scores["single_nclid"],
        "cumulative": scores["cumulative_nclid"]
    }
    state["score_analysis"] = {
        "total_rounds": scores["total_rounds"],
        "score_quality": analyze_score_quality(scores),
        "improvement_suggestions": generate_improvement_suggestions(scores)
    }

    # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ‰©å±•conversationsè¡¨ï¼‰
    save_conversation_with_scores(state)

    return state


def analyze_score_quality(scores: Dict[str, float]) -> str:
    """åˆ†æè¯„åˆ†è´¨é‡"""
    lsm = scores["cumulative_lsm"]
    nclid = scores["cumulative_nclid"]

    if lsm > 0.6 and nclid < 0.3:
        return "ä¼˜ç§€ï¼šé«˜è¯­è¨€åŒæ­¥æ€§ï¼Œä½è¯­ä¹‰è·ç¦»"
    elif lsm > 0.4 and nclid < 0.5:
        return "è‰¯å¥½ï¼šé€‚ä¸­çš„è¯­è¨€åŒæ­¥å’Œè¯­ä¹‰åŒ¹é…"
    elif lsm < 0.3 or nclid > 0.7:
        return "éœ€æ”¹è¿›ï¼šè¯­è¨€åŒæ­¥æ€§æˆ–è¯­ä¹‰åŒ¹é…è¾ƒä½"
    else:
        return "ä¸­ç­‰ï¼šæœ‰æå‡ç©ºé—´"


def generate_improvement_suggestions(scores: Dict[str, float]) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []

    if scores["cumulative_lsm"] < 0.3:
        suggestions.append("AIå¯ä»¥æ›´å¤šåœ°æ¨¡ä»¿ç”¨æˆ·çš„è¯­è¨€é£æ ¼å’Œç”¨è¯ä¹ æƒ¯")

    if scores["cumulative_nclid"] > 0.6:
        suggestions.append("AIå›å¤ä¸ç”¨æˆ·è¾“å…¥çš„è¯­ä¹‰å…³è”åº¦å¯ä»¥æ›´é«˜")

    if scores["total_rounds"] < 5:
        suggestions.append("å¯¹è¯è½®æ•°è¾ƒå°‘ï¼Œå»ºè®®ç»§ç»­å¯¹è¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯„åˆ†")

    return suggestions


def save_conversation_with_scores(state: ChatStateWithScoring):
    """ä¿å­˜å¸¦è¯„åˆ†çš„å¯¹è¯"""
    conn = sqlite3.connect('psychology_ai.db')
    cursor = conn.cursor()

    # æ‰©å±•conversationsè¡¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è¯„åˆ†å­—æ®µï¼‰
    try:
        cursor.execute('ALTER TABLE conversations ADD COLUMN lsm_score REAL')
        cursor.execute('ALTER TABLE conversations ADD COLUMN nclid_score REAL')
        cursor.execute('ALTER TABLE conversations ADD COLUMN cumulative_lsm REAL')
        cursor.execute('ALTER TABLE conversations ADD COLUMN cumulative_nclid REAL')
        cursor.execute('ALTER TABLE conversations ADD COLUMN score_analysis TEXT')
    except sqlite3.OperationalError:
        pass  # å­—æ®µå·²å­˜åœ¨

    # æ’å…¥å¯¹è¯è®°å½•
    cursor.execute('''
        INSERT INTO conversations 
        (user_id, character_id, user_input, ai_response, understanding_score, 
         cbt_stage, emotion_detected, lsm_score, nclid_score, 
         cumulative_lsm, cumulative_nclid, score_analysis, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
    ''', (
        state["user_id"], state["character_id"], state["user_input"],
        state["ai_response"], state["understanding_score"],
        state["cbt_stage"], state["emotion"],
        state["lsm_scores"]["single"], state["nclid_scores"]["single"],
        state["lsm_scores"]["cumulative"], state["nclid_scores"]["cumulative"],
        json.dumps(state["score_analysis"], ensure_ascii=False)
    ))

    conn.commit()
    conn.close()


# ============ ç”¨æˆ·ç•Œé¢é›†æˆ ============

class EnhancedLangGraphAI:
    """å¢å¼ºçš„LangGraph AI - åŒ…å«è¯„åˆ†ç³»ç»Ÿ"""

    def __init__(self):
        from backend.workflow.graph import create_workflow  # ä½¿ç”¨å¢å¼ºçš„å·¥ä½œæµ
        self.workflow = create_workflow()
        self.scorer = LSMNCLIDScorer()

    def chat_with_character(self, user_id: str, user_input: str, character_id: str = "nomi") -> dict:
        """ä¸AIè§’è‰²å¯¹è¯ - åŒ…å«è¯„åˆ†ä¿¡æ¯"""

        # æ„å»ºå¢å¼ºçš„çŠ¶æ€
        initial_state = ChatStateWithScoring(
            user_id=user_id,
            user_input=user_input,
            character_id=character_id,
            emotion="",
            cbt_stage=1,
            ai_response="",
            understanding_score=0.0,
            lsm_scores={},
            nclid_scores={},
            score_analysis={},
            metadata={}
        )

        # æ‰§è¡Œå·¥ä½œæµ
        result_state = self.workflow.invoke(initial_state)

        # è¿”å›å¢å¼ºçš„ç»“æœ
        return {
            'character': result_state["character_id"],
            'response': result_state["ai_response"],
            'understanding_score': result_state["understanding_score"],
            'emotion_detected': result_state["emotion"],
            'cbt_stage': result_state["cbt_stage"],

            # æ–°å¢è¯„åˆ†ä¿¡æ¯
            'lsm_single': result_state["lsm_scores"]["single"],
            'lsm_cumulative': result_state["lsm_scores"]["cumulative"],
            'nclid_single': result_state["nclid_scores"]["single"],
            'nclid_cumulative': result_state["nclid_scores"]["cumulative"],
            'score_quality': result_state["score_analysis"]["score_quality"],
            'total_rounds': result_state["score_analysis"]["total_rounds"],
            'improvement_suggestions': result_state["score_analysis"]["improvement_suggestions"]
        }

    def get_user_score_report(self, user_id: str, days: int = 7) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·è¯„åˆ†æŠ¥å‘Š"""
        return self.scorer.get_user_score_summary(user_id, days)


# ============ å‘½ä»¤è¡Œç•Œé¢å¢å¼º ============

def enhanced_main():
    """å¢å¼ºçš„ä¸»ç¨‹åº - æ˜¾ç¤ºè¯„åˆ†ä¿¡æ¯"""
    print("ğŸ§  å¿ƒç†AIåŠ©æ‰‹ - å¸¦LSM&nCLiDè¯„åˆ†ç³»ç»Ÿ")
    print("=" * 60)

    # åˆå§‹åŒ–å¢å¼ºç³»ç»Ÿ
    ai_system = EnhancedLangGraphAI()

    user_id = input("è¯·è¾“å…¥ç”¨æˆ·åï¼š").strip() or "test_user"
    print(f"æ¬¢è¿ï¼Œ{user_id}ï¼")

    # æ˜¾ç¤ºå†å²è¯„åˆ†æ‘˜è¦
    score_summary = ai_system.get_user_score_report(user_id)
    if score_summary["total_conversations"] > 0:
        print(f"\nğŸ“Š ä½ çš„è¯„åˆ†æ‘˜è¦ï¼ˆæœ€è¿‘7å¤©ï¼‰ï¼š")
        print(f"   å¹³å‡LSM: {score_summary['avg_lsm']:.4f}")
        print(f"   å¹³å‡nCLiD: {score_summary['avg_nclid']:.4f}")
        print(f"   è¯„åˆ†è¶‹åŠ¿: {score_summary['score_trend']}")
        print(f"   å¯¹è¯æ€»æ•°: {score_summary['total_conversations']}")

    current_character = "nomi"
    print(f"\nå½“å‰è§’è‰²ï¼š{current_character}")

    while True:
        print(f"\n[{current_character}] ä½ ï¼š", end=" ")
        user_input = input().strip()

        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            # æ˜¾ç¤ºæœ€ç»ˆè¯„åˆ†æŠ¥å‘Š
            final_report = ai_system.get_user_score_report(user_id)
            print(f"\nğŸ“ˆ æœ€ç»ˆè¯„åˆ†æŠ¥å‘Šï¼š")
            print(f"   LSM (è¯­è¨€åŒæ­¥): {final_report['latest_lsm']:.4f}")
            print(f"   nCLiD (è¯­ä¹‰è·ç¦»): {final_report['latest_nclid']:.4f}")
            print(f"   å¯¹è¯è´¨é‡: {analyze_score_quality(final_report)}")
            break

        # å¤„ç†ç‰¹æ®Šå‘½ä»¤
        if user_input == '/scores':
            report = ai_system.get_user_score_report(user_id)
            print(f"ğŸ“Š è¯„åˆ†æŠ¥å‘Šï¼š")
            print(f"   å½“å‰LSM: {report['latest_lsm']:.4f}")
            print(f"   å½“å‰nCLiD: {report['latest_nclid']:.4f}")
            print(f"   å¹³å‡ç†è§£æ„Ÿ: {report['avg_understanding']:.2f}")
            continue

        if not user_input:
            continue

        # å¤„ç†å¯¹è¯
        try:
            result = ai_system.chat_with_character(user_id, user_input, current_character)

            # æ˜¾ç¤ºAIå›å¤
            print(f"\nğŸ¤– {current_character}ï¼š{result['response']}")

            # æ˜¾ç¤ºè¯¦ç»†è¯„åˆ†ä¿¡æ¯
            print(f"\nğŸ“Š æœ¬è½®è¯„åˆ†ï¼š")
            print(f"   ç†è§£æ„Ÿ: {result['understanding_score']:.2f}")
            print(f"   LSM: {result['lsm_single']:.4f} (ç´¯ç§¯: {result['lsm_cumulative']:.4f})")
            print(f"   nCLiD: {result['nclid_single']:.4f} (ç´¯ç§¯: {result['nclid_cumulative']:.4f})")
            print(f"   è´¨é‡è¯„ä»·: {result['score_quality']}")
            print(f"   å¯¹è¯è½®æ•°: {result['total_rounds']}")

            # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
            if result['improvement_suggestions']:
                print(f"ğŸ’¡ æ”¹è¿›å»ºè®®: {'; '.join(result['improvement_suggestions'])}")

        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼š{str(e)}")


if __name__ == "__main__":
    enhanced_main()