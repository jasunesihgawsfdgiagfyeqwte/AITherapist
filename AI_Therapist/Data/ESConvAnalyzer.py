#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import jieba
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
from collections import Counter, defaultdict
import sqlite3
from datetime import datetime
import re
import os
from sentence_transformers import SentenceTransformer, util

class ESConvAnalyzer:
    """ESConvæ•°æ®æ·±åº¦åˆ†æå™¨"""

    def __init__(self, data_path: str):
        self.data_path = data_path
        self.data = self.load_data()
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

        # åˆå§‹åŒ–ä¸­æ–‡åŠŸèƒ½è¯åº“
        self.function_words = {
            "pronouns": [
                "i", "you", "me", "my", "your", "we", "they", "he", "she", "him", "her",
                "ours", "yourself", "myself", "their", "our", "his", "them", "its"
            ],
            "negations": [
                "not", "no", "never", "don", "didn", "won", "canâ€™t", "couldn", "wasn", "donâ€™t"
            ],
            "emotion_markers": [
                "feel", "feeling", "hope", "happy", "sorry", "understand", "love", "hate",
                "care", "worried", "hard", "sad", "angry", "upset", "frustrated", "nervous",
                "depressed", "anxious", "okay", "support", "great", "good", "bad", "better"
            ],
            "emotion_intensifiers": [
                "very", "really", "so", "too", "just", "definitely", "much", "more", "quite",
                "also", "even", "still", "only", "totally", "completely", "absolutely"
            ],
            "temporal_markers": [
                "now", "today", "always", "when", "before", "after", "soon", "sometimes",
                "still", "again", "long", "never", "year", "day", "years"
            ],
            "modal_particles": [
                "maybe", "well", "oh", "sure", "ok", "yeah", "thank", "thanks", "hmm",
                "um", "like", "hello", "hi"
            ]
        }

        # åˆå¹¶æ‰€æœ‰åŠŸèƒ½è¯
        self.all_function_words = set()
        for words in self.function_words.values():
            self.all_function_words.update(words)

    def load_data(self) -> List[Dict]:
        """åŠ è½½ESConvæ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {self.data_path}")
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡å¯¹è¯")
        return data

    def analyze_data_distribution(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        print("ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒ...")

        # æƒ…ç»ªç±»å‹åˆ†å¸ƒ
        emotion_types = [conv.get('emotion_type', 'unknown') for conv in self.data]
        emotion_dist = Counter(emotion_types)

        # é—®é¢˜ç±»å‹åˆ†å¸ƒ
        problem_types = [conv.get('problem_type', 'unknown') for conv in self.data]
        problem_dist = Counter(problem_types)

        # å¯¹è¯é•¿åº¦åˆ†å¸ƒ
        dialog_lengths = [len(conv.get('dialog', [])) for conv in self.data]

        # æ”¯æŒç­–ç•¥åˆ†æ
        strategies = []
        feedback_scores = []

        for conv in self.data:
            dialog = conv.get('dialog', [])
            for turn in dialog:
                if turn.get('speaker') == 'supporter':
                    strategy = turn.get('annotation', {}).get('strategy')
                    if strategy:
                        strategies.append(strategy)

                # æ”¶é›†åé¦ˆåˆ†æ•°
                if 'feedback' in turn.get('annotation', {}):
                    try:
                        score = int(turn['annotation']['feedback'])
                        feedback_scores.append(score)
                    except:
                        pass

        strategy_dist = Counter(strategies)

        # æƒ…ç»ªæ”¹å–„åˆ†æ
        emotion_improvements = []
        for conv in self.data:
            survey = conv.get('survey_score', {}).get('seeker', {})
            if 'initial_emotion_intensity' in survey and 'final_emotion_intensity' in survey:
                try:
                    initial = int(survey['initial_emotion_intensity'])
                    final = int(survey['final_emotion_intensity'])
                    improvement = initial - final
                    emotion_improvements.append(improvement)
                except:
                    pass

        analysis = {
            'total_conversations': len(self.data),
            'emotion_distribution': dict(emotion_dist),
            'problem_distribution': dict(problem_dist),
            'dialog_length_stats': {
                'mean': np.mean(dialog_lengths),
                'std': np.std(dialog_lengths),
                'min': np.min(dialog_lengths),
                'max': np.max(dialog_lengths),
                'median': np.median(dialog_lengths)
            },
            'strategy_distribution': dict(strategy_dist.most_common(10)),
            'feedback_stats': {
                'mean': np.mean(feedback_scores) if feedback_scores else 0,
                'count': len(feedback_scores),
                'distribution': dict(Counter(feedback_scores))
            },
            'emotion_improvement_stats': {
                'mean': np.mean(emotion_improvements) if emotion_improvements else 0,
                'positive_count': len([x for x in emotion_improvements if x > 0]),
                'total_count': len(emotion_improvements)
            }
        }

        return analysis

    def calculate_conversation_quality_score(self, conv: Dict) -> float:
        """è®¡ç®—å¯¹è¯è´¨é‡åˆ†æ•°"""
        dialog = conv.get('dialog', [])

        # åŸºç¡€æƒé‡
        weights = {
            'length': 0.2,  # å¯¹è¯é•¿åº¦
            'balance': 0.2,  # å¯¹è¯å¹³è¡¡æ€§
            'strategies': 0.25,  # ç­–ç•¥å¤šæ ·æ€§
            'feedback': 0.25,  # ç”¨æˆ·åé¦ˆ
            'improvement': 0.1  # æƒ…ç»ªæ”¹å–„
        }

        # 1. å¯¹è¯é•¿åº¦è¯„åˆ† (6-20è½®ä¸ºæœ€ä½³)
        length = len(dialog)
        if 6 <= length <= 20:
            length_score = 1.0
        elif length < 6:
            length_score = length / 6
        else:
            length_score = max(0.5, 20 / length)

        # 2. å¯¹è¯å¹³è¡¡æ€§è¯„åˆ†
        seeker_count = len([t for t in dialog if t.get('speaker') == 'seeker'])
        supporter_count = len([t for t in dialog if t.get('speaker') == 'supporter'])
        if max(seeker_count, supporter_count) > 0:
            balance_score = min(seeker_count, supporter_count) / max(seeker_count, supporter_count)
        else:
            balance_score = 0

        # 3. ç­–ç•¥å¤šæ ·æ€§è¯„åˆ†
        strategies = set()
        for turn in dialog:
            if turn.get('speaker') == 'supporter':
                strategy = turn.get('annotation', {}).get('strategy')
                if strategy:
                    strategies.add(strategy)

        strategy_score = min(len(strategies) / 5, 1.0)  # 5ç§ç­–ç•¥ä¸ºæ»¡åˆ†

        # 4. ç”¨æˆ·åé¦ˆè¯„åˆ†
        feedback_scores = []
        for turn in dialog:
            if 'feedback' in turn.get('annotation', {}):
                try:
                    score = int(turn['annotation']['feedback'])
                    feedback_scores.append(score)
                except:
                    pass

        if feedback_scores:
            feedback_score = np.mean(feedback_scores) / 5  # æ ‡å‡†åŒ–åˆ°0-1
        else:
            feedback_score = 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°

        # 5. æƒ…ç»ªæ”¹å–„è¯„åˆ†
        survey = conv.get('survey_score', {}).get('seeker', {})
        improvement_score = 0.5  # é»˜è®¤åˆ†æ•°

        if 'initial_emotion_intensity' in survey and 'final_emotion_intensity' in survey:
            try:
                initial = int(survey['initial_emotion_intensity'])
                final = int(survey['final_emotion_intensity'])
                improvement = initial - final
                improvement_score = min(max(improvement / 4, 0), 1)  # 4åˆ†æ”¹å–„ä¸ºæ»¡åˆ†
            except:
                pass

        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = (
                weights['length'] * length_score +
                weights['balance'] * balance_score +
                weights['strategies'] * strategy_score +
                weights['feedback'] * feedback_score +
                weights['improvement'] * improvement_score
        )

        return round(total_score, 3)

    def calculate_nclid(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„nCLiDè¯­ä¹‰è·ç¦»ï¼ˆ1 - ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        emb1 = self.embedding_model.encode(text1, convert_to_tensor=True)
        emb2 = self.embedding_model.encode(text2, convert_to_tensor=True)
        similarity = util.pytorch_cos_sim(emb1, emb2).item()
        return round(1 - similarity, 4)

    def select_high_quality_conversations(self, target_count: int = 1000) -> List[Dict]:
        """é€‰æ‹©é«˜è´¨é‡å¯¹è¯"""
        print(f"ğŸ” ç­›é€‰é«˜è´¨é‡å¯¹è¯ï¼Œç›®æ ‡æ•°é‡: {target_count}")

        # ä¸ºæ¯ä¸ªå¯¹è¯è®¡ç®—è´¨é‡åˆ†æ•°
        scored_conversations = []

        for i, conv in enumerate(self.data):
            if i % 100 == 0:
                print(f"è¿›åº¦: {i}/{len(self.data)}")

            try:
                quality_score = self.calculate_conversation_quality_score(conv)
                scored_conversations.append((conv, quality_score))
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å¯¹è¯ {i} æ—¶å‡ºé”™: {e}")
                continue

        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        scored_conversations.sort(key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰Nä¸ª
        selected = [item[0] for item in scored_conversations[:target_count]]

        print(f"âœ… æˆåŠŸç­›é€‰ {len(selected)} æ¡é«˜è´¨é‡å¯¹è¯")
        print(f"è´¨é‡åˆ†æ•°èŒƒå›´: {scored_conversations[0][1]:.3f} - {scored_conversations[target_count - 1][1]:.3f}")

        return selected

    def extract_function_words(self, text: str) -> List[str]:
        """æå–åŠŸèƒ½è¯"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\u4e00-\u9fff\w\s]', '', text.lower())

        # åˆ†è¯
        tokens = jieba.lcut(text)

        # æå–åŠŸèƒ½è¯
        function_words = [token for token in tokens if token in self.all_function_words]

        return function_words

    def calculate_lsm_score(self, seeker_text: str, supporter_text: str) -> float:
        """è®¡ç®—LSMåˆ†æ•°"""
        seeker_words = set(self.extract_function_words(seeker_text))
        supporter_words = set(self.extract_function_words(supporter_text))

        if not seeker_words and not supporter_words:
            return 0.0

        overlap = len(seeker_words & supporter_words)
        total = len(seeker_words | supporter_words)

        return overlap / total if total > 0 else 0.0

    def process_conversations_basic(self, conversations: List[Dict]) -> List[Dict]:
        """åŸºç¡€å¤„ç†å¯¹è¯ï¼ˆä¸ä½¿ç”¨è¯­ä¹‰æ¨¡å‹ï¼‰"""
        print(f"âš™ï¸ åŸºç¡€å¤„ç† {len(conversations)} æ¡å¯¹è¯...")

        processed = []

        for i, conv in enumerate(conversations):
            if i % 50 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{len(conversations)}")

            try:
                dialog = conv.get('dialog', [])

                # æå–å¯¹è¯å¯¹å¹¶è®¡ç®—LSM
                pairs = []
                lsm_scores = []
                # æå–Nclidåˆ†æ•°
                nclid_scores = []

                for j in range(len(dialog) - 1):
                    current = dialog[j]
                    next_turn = dialog[j + 1]

                    if current.get('speaker') == 'seeker' and next_turn.get('speaker') == 'supporter':
                        seeker_text = current.get('content', '')
                        supporter_text = next_turn.get('content', '')

                        if seeker_text.strip() and supporter_text.strip():
                            lsm = self.calculate_lsm_score(seeker_text, supporter_text)
                            nclid = self.calculate_nclid(seeker_text, supporter_text)

                            pairs.append((seeker_text, supporter_text))
                            lsm_scores.append(lsm)
                            nclid_scores.append(nclid)

                # è®¡ç®—ç»Ÿè®¡é‡
                processed_conv = {
                    'id': i,
                    'emotion_type': conv.get('emotion_type', ''),
                    'problem_type': conv.get('problem_type', ''),
                    'situation': conv.get('situation', ''),
                    'total_turns': len(dialog),
                    'conversation_pairs': len(pairs),
                    'lsm_scores': lsm_scores,
                    'avg_lsm': np.mean(lsm_scores) if lsm_scores else 0.0,
                    'avg_nclid': np.mean(nclid_scores) if nclid_scores else 1.0,
                    'quality_score': self.calculate_conversation_quality_score(conv),
                    'original_dialog': dialog,
                    'processed_at': datetime.now().isoformat()
                }

                processed.append(processed_conv)

            except Exception as e:
                print(f"âš ï¸ å¤„ç†å¯¹è¯ {i} æ—¶å‡ºé”™: {e}")
                continue

        print(f"âœ… åŸºç¡€å¤„ç†å®Œæˆï¼Œå…± {len(processed)} æ¡å¯¹è¯")
        return processed

    def save_results(self, processed_conversations: List[Dict],
                     analysis: Dict[str, Any], output_dir: str = "output"):
        def convert_to_builtin(obj):
            if isinstance(obj, dict):
                return {k: convert_to_builtin(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_builtin(v) for v in obj]
            elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
                return int(obj)
            elif isinstance(obj, (np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        """ä¿å­˜å¤„ç†ç»“æœ"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ° {output_dir} ç›®å½•...")

        # 1. ä¿å­˜å¤„ç†åçš„å¯¹è¯æ•°æ®
        conversations_file = os.path.join(output_dir, 'processed_conversations.json')
        with open(conversations_file, 'w', encoding='utf-8') as f:
            json.dump(processed_conversations, f, ensure_ascii=False, indent=2)

        # 2. ä¿å­˜åˆ†ææŠ¥å‘Š
        analysis_file = os.path.join(output_dir, 'data_analysis_report.json')
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(convert_to_builtin(analysis), f, ensure_ascii=False, indent=2)

        # 3. ä¿å­˜SQLiteæ•°æ®åº“
        db_file = os.path.join(output_dir, 'esconv_processed.db')
        self.save_to_database(processed_conversations, db_file)

        # 4. ç”Ÿæˆç®€è¦ç»Ÿè®¡æŠ¥å‘Š
        summary_file = os.path.join(output_dir, 'summary_report.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("ESConvæ•°æ®å¤„ç†æ‘˜è¦æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"åŸå§‹æ•°æ®é‡: {analysis['total_conversations']} æ¡å¯¹è¯\n")
            f.write(f"å¤„ç†åæ•°æ®é‡: {len(processed_conversations)} æ¡å¯¹è¯\n")
            f.write(f"å¹³å‡å¯¹è¯é•¿åº¦: {analysis['dialog_length_stats']['mean']:.1f} è½®\n")
            f.write(f"å¹³å‡LSMåˆ†æ•°: {np.mean([conv['avg_lsm'] for conv in processed_conversations]):.4f}\n")
            f.write(f"å¹³å‡è´¨é‡åˆ†æ•°: {np.mean([conv['quality_score'] for conv in processed_conversations]):.4f}\n\n")

            f.write("æƒ…ç»ªç±»å‹åˆ†å¸ƒ:\n")
            for emotion, count in analysis['emotion_distribution'].items():
                f.write(f"  {emotion}: {count}\n")

            f.write("\næ”¯æŒç­–ç•¥åˆ†å¸ƒ (Top 5):\n")
            for strategy, count in list(analysis['strategy_distribution'].items())[:5]:
                f.write(f"  {strategy}: {count}\n")

        print(f"âœ… ç»“æœä¿å­˜å®Œæˆ:")
        print(f"  - {conversations_file}")
        print(f"  - {analysis_file}")
        print(f"  - {db_file}")
        print(f"  - {summary_file}")

    def save_to_database(self, conversations: List[Dict], db_path: str):
        """ä¿å­˜åˆ°SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # åˆ›å»ºè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY,
                emotion_type TEXT,
                problem_type TEXT,
                situation TEXT,
                total_turns INTEGER,
                conversation_pairs INTEGER,
                avg_lsm REAL,
                avg_nclid REAL,
                quality_score REAL,
                dialog_json TEXT,
                processed_at TEXT
            )
        ''')

        # æ’å…¥æ•°æ®
        for conv in conversations:
            cursor.execute('''
                INSERT INTO conversations 
                (emotion_type, problem_type, situation, total_turns, 
                 conversation_pairs, avg_lsm, avg_nclid, quality_score, dialog_json, processed_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                conv['emotion_type'],
                conv['problem_type'],
                conv['situation'],
                conv['total_turns'],
                conv['conversation_pairs'],
                conv['avg_lsm'],
                conv['avg_nclid'],  # ğŸ‘ˆ åŠ å…¥è¿™ä¸€é¡¹
                conv['quality_score'],
                json.dumps(conv['original_dialog'], ensure_ascii=False),
                conv['processed_at']
            ))

        conn.commit()
        conn.close()


def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    print("ğŸš€ ESConvæ•°æ®åˆ†æä¸å¤„ç†Pipelineå¯åŠ¨")
    print("=" * 60)

    # æ•°æ®è·¯å¾„ - è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è°ƒæ•´
    data_path = "ESConv.json"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®æ–‡ä»¶è·¯å¾„

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = ESConvAnalyzer(data_path)

    # 1. åˆ†ææ•°æ®åˆ†å¸ƒ
    print("\nğŸ“Š æ­¥éª¤1: åˆ†ææ•°æ®åˆ†å¸ƒ")
    analysis = analyzer.analyze_data_distribution()

    # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
    print(f"æ€»å¯¹è¯æ•°: {analysis['total_conversations']}")
    print(f"æƒ…ç»ªç±»å‹: {list(analysis['emotion_distribution'].keys())}")
    print(f"å¹³å‡å¯¹è¯é•¿åº¦: {analysis['dialog_length_stats']['mean']:.1f} è½®")
    print(f"ç”¨æˆ·åé¦ˆå¹³å‡åˆ†: {analysis['feedback_stats']['mean']:.2f}")

    # 2. ç­›é€‰é«˜è´¨é‡å¯¹è¯
    print("\nğŸ” æ­¥éª¤2: ç­›é€‰é«˜è´¨é‡å¯¹è¯")
    high_quality_conversations = analyzer.select_high_quality_conversations(target_count=1000)

    # 3. åŸºç¡€å¤„ç†ï¼ˆLSMè®¡ç®—ï¼‰
    print("\nâš™ï¸ æ­¥éª¤3: åŸºç¡€å¤„ç†å’ŒLSMè®¡ç®—")
    processed_conversations = analyzer.process_conversations_basic(high_quality_conversations)

    # 4. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­¥éª¤4: ä¿å­˜å¤„ç†ç»“æœ")
    analyzer.save_results(processed_conversations, analysis)

    # 5. æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\nğŸ‰ å¤„ç†å®Œæˆ! æ€»ç»“:")
    print("=" * 60)
    print(f"âœ… åŸå§‹æ•°æ®: {len(analyzer.data)} æ¡å¯¹è¯")
    print(f"âœ… ç­›é€‰å: {len(high_quality_conversations)} æ¡é«˜è´¨é‡å¯¹è¯")
    print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_conversations)} æ¡å¯¹è¯")
    print(f"âœ… å¹³å‡LSMåˆ†æ•°: {np.mean([conv['avg_lsm'] for conv in processed_conversations]):.4f}")
    print(f"âœ… å¹³å‡è´¨é‡åˆ†æ•°: {np.mean([conv['quality_score'] for conv in processed_conversations]):.4f}")

    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - output/processed_conversations.json")
    print("  - output/data_analysis_report.json")
    print("  - output/esconv_processed.db")
    print("  - output/summary_report.txt")


if __name__ == "__main__":
    main()